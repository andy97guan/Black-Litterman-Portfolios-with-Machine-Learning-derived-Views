\documentclass{article}

\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{enumerate}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{textcomp}
\usepackage[cache=false]{minted}
\usepackage{geometry}
\usepackage{float}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage[colorlinks,linkcolor=blue,anchorcolor=red,citecolor=green]{hyperref}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\geometry{left=1cm,right=1cm,top=1cm,bottom=1.5cm}

\title{Feature Selection}
\author{Ruizhe He \\ U21165771}
\date{\today}

\begin{document}

\maketitle

\section{Label Generation}

As mentioned before, we need a $Q$ matrix for each period as the input of our Black-Litterman model. By transformation, the equation of $Q$ is:

\begin{equation}
Q_t(k)=(P_t \pi_t)(k)+\eta_k\sqrt{(P_t \sigma_t P_t^{'})(k,k)} \quad\quad k=1,2,...,K
\label{Qeta}
\end{equation}

We are going to apply machine learning methods on $\eta$ above. For each period $t$, we will generate a vector of $\eta$, and $k$ here is the $k$-the view. We use rolling classifier (150 previous data) to fit a model and use it to predict next week's $\eta$.

To use machine learning, we need to first generate our label. Here we define $\eta$ into 4 classes: $\eta\in\{-2,-1,1,2\}$, which is "Very Bearish", "Bearish", "Bullish" and "Very Bullish" respectively. We now need two classifiers to classify them. The first one $Y_1$ is the sign and the second one $Y_2$ is absolute value. The definition is as below:

\begin{equation}
Y_1=\left\{
	\begin{aligned}
	&-1, \text{if sign of excess return of next week's is negative} \\
	&+1, \text{if sign of excess return of next week's is positive} \\
	\end{aligned}
\right.
\label{y1}
\end{equation}

\begin{equation}
Y_2=\left\{
	\begin{aligned}
	&1, \text{if }z_t=\frac{r_t-\bar{r}_{t,3}}{\sigma_{t,3}}\leq 1\\
	&2, \text{if }z_t=\frac{r_t-\bar{r}_{t,3}}{\sigma_{t,3}}>1 \\
	\end{aligned}
\right.
\label{y2}
\end{equation}

Finally, we multiply $Y_1$ and $Y_2$ to generate the final label of $\eta$.

\section{Feature Generation}

Feature is very important for machine learning. Good features will help to fit model with better performance. Here we choose different kinds of indicators to consider different aspects that influence the performance of the ETFs. Different features are chosen as follows:

\begin{enumerate}[$\bullet$]

\item Returns and excess returns of 4 ETFs and S$\&$P 500, VIX, and 10 Year treasury note yield index.

\item Rate of change of volume of 4 ETFs.

\item Momentum indicators: CCI, RSI and Fast Stochastic Oscillator of 4 ETFs and S$\&$P 500, VIX, and 10 Year treasury note yield index.

\item Trend indicators: SMA, EMA and MACD of 4 ETFs and S$\&$P 500, VIX, and 10 Year treasury note yield index.

\item Volatility indicators: Bollinger Bands and Volatility Ratio of 4 ETFs and S$\&$P 500, VIX, and 10 Year treasury note yield index.

\end{enumerate}

We put the indicators of different ETF's together to consider the correlation between those ETF's. We also calculated previous 1-4 weeks' period of indicators and their 1-3 lags to consider time series issue. Eventually, we generate 1324 different features.

\section{Feature Selection}

After generating those 1324 features, we want to select the important features and use them to fit our machine learning model to avoid overfitting. For each ETF's 2 different classifiers, we will process a feature selection. We select 10 most important features and therefore we will generate 8 feature matrices.

While processing feature selection, we use the Extra Tree Classifier. Extra Tree Classifier is a version of Random Forest. It is a kind of ensemble learning algorithms which gathers the result of multiple independent decision trees and combine them into the output of feature ranking. The difference between Extra Tree Classifier and Random Forest is that when splitting the nodes, Random Forest select the best feature based on some criteria, such as information gain and Gini Index, but Extra Tree Classifier select the beat feature randomly. Therefore, Extra Tree Classifier has better generalization probability than Random Forest. This is why we choose it to be our feature selection method.

When label generation and feature selection is finished, we can move forward to train our machine learning model and give out the prediction of $\eta$.

\end{document}
